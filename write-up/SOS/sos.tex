\documentclass{article}
\usepackage[]{amsmath}
\usepackage{amsthm} 
\usepackage{amsbsy}
% \newtheorem*{remark}{Main Result}
\usepackage{breqn}
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} % for postscript graphics files
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage[margin=1in]{geometry}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\newtheorem*{remark}{Remark}
% \usepackage{todonotes}
% \usepackage{algorithmic}
% \usepackage[numbered, framed]{mcode}
% \newtheorem{remark}{Remark}
\begin{document}




\section{Composable Verification for Polynomial System} % (fold)
\label{sec:composable_verification_for_polynomial_system}
Consider a polynomial system $\dot x=f(x)$ where the state vector $x\in \mathbb{R}^m$. Partition $x$ into $n$ disjoint parts $x=[x_1;x_2;\dots;x_n]$ where $x_i \in \mathbb{R}^{m_i}$ and $\sum\limits_{i}{m_i}=m$, and write the dynamics of the sub-systems as $\dot{x_i}=f_i(x_i)+g_i(x)$, $i=1,2,\dots,n$, that is, $f_i$ is a polynomial function purely of $x_i$, whereas $g_i$ is a function of the original system states $x$. For simplicity, group any constant terms into $f_i$, so then $g_i$ consists of all the cross-product terms.

$\lnot$


If 
then $V=\sum\limits_{i}{V_i}$ is a Lyapunov function for the original system, i.e. $\frac{\partial{V}}{x}<0$

$\frac{\partial{V}}{x}=\sum\limits_{i}{\frac{\partial{V_i}}{x_i}\dot{x_i}}=\sum\limits_{i}{\frac{\partial{V_i}}{x_i}(f_i(x_i)+g_i(x)})$


\section{Polynomial extension of Schur complement} % (fold)
\label{sec:polynomial_extension_to_schur_complement}
Consider the following problem: Given a polynomial function $p(x)$, which is known to be sum-of-squares, i.e. $p(x)=m_p'(x)Pm_p(x)$ where $m_p(x)$ is an appropriate monomial basis, and $P\succ 0$, the problem is to find a polynomial function $f(x)=c'*m_f(x)$ of a fixed degree $d$ such that $p(x)-f(x)^2$ is SOS. At first glance, due to the constraint involving terms that is quadratic in the decision variables coming from $f(x)^2$, illegal in the low-level SDP solvers' problem setup. Let us take a closer investigation. Finding an SOS decomposition of $p(x)-f(x)^2$ is equivalent to finding a $Q \succ 0$ such that $p(x)-f(x)^2=m'(x)Qm(x)$ where $m(x)$ is a monomial basis. Since $p(x)$ is known, and $f(x)$ is of a fixed degree, $m(x)$ is known, and it contains at least all the elements in $m_p(x)$ and $m_f(x)$. Hence, $p(x)$ can be re-written as $p(x)=m'(x)\tilde{P}m(x)$, where $\tilde{P}$ is $P$ padded with zeros accounting for the added element in $m(x)$ and is guaranteed to be p.s.d. as well. Similarly, $f(x)=\tilde{c}'*m(x)$ where $\tilde{c}$, the vector of coefficients, is the padded decision variables. Next, since $f(x)$ is a scalar polynomial function, $f(x)^2=f'(x)*f(x)=m'(x)*\tilde{c}*\tilde{c}'*m(x)$. Combining everything together, we have $p(x)-f(x)^2=m'(x)[\tilde{P}-\tilde{c}'*\tilde{c}]m(x)$. In other words, we've transformed the problem of finding $Q$ into finding $\tilde{c}$ such that $(\tilde{P}-\tilde{c}'*\tilde{c})\succ 0$ where $\tilde{P}$ is known and is p.s.d. This can be convexified by the well-known Schur complement trick as:
\[
\begin{bmatrix}
  \tilde{P} & \tilde{c}\\
  \tilde{c}' & 1 \\
\end{bmatrix}\succ 0 \], which can be readily handled by any SDP solvers. 


As a side note, the result above exploits the inherent equivalence of an SOS decomposition and an SDP. In fact, an SDP constraint is nothing but an SOS decomposition with a monomial basis capped at degree 1. From this point of view, other convexify tricks developed by the LMI community may be straightforwardly generalizable to SOS too. A precise proof is of course still required to make any claim. 
% section polynomial_extension_to_schur_complement (end)


\end{document}


